{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from functools import reduce\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from torch import cuda"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-22T10:39:03.564695Z",
     "end_time": "2023-08-22T10:39:12.649379Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "model_path = \"../models/saved_instances/yolov8x-seg.pt\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-22T10:39:12.649379Z",
     "end_time": "2023-08-22T10:39:12.663773Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "{0: 'person',\n 1: 'bicycle',\n 2: 'car',\n 3: 'motorcycle',\n 4: 'airplane',\n 5: 'bus',\n 6: 'train',\n 7: 'truck',\n 8: 'boat',\n 9: 'traffic light',\n 10: 'fire hydrant',\n 11: 'stop sign',\n 12: 'parking meter',\n 13: 'bench',\n 14: 'bird',\n 15: 'cat',\n 16: 'dog',\n 17: 'horse',\n 18: 'sheep',\n 19: 'cow',\n 20: 'elephant',\n 21: 'bear',\n 22: 'zebra',\n 23: 'giraffe',\n 24: 'backpack',\n 25: 'umbrella',\n 26: 'handbag',\n 27: 'tie',\n 28: 'suitcase',\n 29: 'frisbee',\n 30: 'skis',\n 31: 'snowboard',\n 32: 'sports ball',\n 33: 'kite',\n 34: 'baseball bat',\n 35: 'baseball glove',\n 36: 'skateboard',\n 37: 'surfboard',\n 38: 'tennis racket',\n 39: 'bottle',\n 40: 'wine glass',\n 41: 'cup',\n 42: 'fork',\n 43: 'knife',\n 44: 'spoon',\n 45: 'bowl',\n 46: 'banana',\n 47: 'apple',\n 48: 'sandwich',\n 49: 'orange',\n 50: 'broccoli',\n 51: 'carrot',\n 52: 'hot dog',\n 53: 'pizza',\n 54: 'donut',\n 55: 'cake',\n 56: 'chair',\n 57: 'couch',\n 58: 'potted plant',\n 59: 'bed',\n 60: 'dining table',\n 61: 'toilet',\n 62: 'tv',\n 63: 'laptop',\n 64: 'mouse',\n 65: 'remote',\n 66: 'keyboard',\n 67: 'cell phone',\n 68: 'microwave',\n 69: 'oven',\n 70: 'toaster',\n 71: 'sink',\n 72: 'refrigerator',\n 73: 'book',\n 74: 'clock',\n 75: 'vase',\n 76: 'scissors',\n 77: 'teddy bear',\n 78: 'hair drier',\n 79: 'toothbrush'}"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ALL_NAMES:\n",
    "# YOLO(model_path).names"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-22T10:39:12.663773Z",
     "end_time": "2023-08-22T10:39:12.910039Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "CLASSES_IDS = [\n",
    "    2, # Car\n",
    "    7, # Truck\n",
    "    5, # Bus\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-22T10:39:12.910039Z",
     "end_time": "2023-08-22T10:39:12.917981Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "device = \"gpu\" if cuda.is_available() else \"cpu\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-22T10:39:12.917981Z",
     "end_time": "2023-08-22T10:39:12.928818Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class YOLOSegmentation:\n",
    "    def __init__(self, model_path, device):\n",
    "        self.model = YOLO(model_path,)\n",
    "        self.device = device\n",
    "\n",
    "    def detect(self, img):\n",
    "        height, width, channels = img.shape\n",
    "\n",
    "        results = self.model.predict(\n",
    "            source=img.copy(), save=False, save_txt=False, device=self.device,\n",
    "            conf=0.3  # Default confidence is 0.25. We need more so that we will skip various trimmed machines that we don't expect to get\n",
    "        )\n",
    "        if results is None:\n",
    "            return [], [], [], []\n",
    "\n",
    "        result = results[0]\n",
    "        segmentation_contours_idx = []\n",
    "        for seg in result.masks.xyn:\n",
    "            seg[:, 0] *= width\n",
    "            seg[:, 1] *= height\n",
    "            segment = np.array(seg, dtype=np.int32)\n",
    "            segmentation_contours_idx.append(segment)\n",
    "        bboxes = np.array(result.boxes.xyxy.to('cpu'), dtype=\"int\")\n",
    "        class_ids = np.array(result.boxes.cls.to('cpu'), dtype=\"int\")\n",
    "        scores = np.array(result.boxes.conf.to('cpu'), dtype=\"float\").round(2)\n",
    "\n",
    "        # bboxes = np.array(result.boxes.xyxy.cpu(), dtype=\"int\")\n",
    "        # class_ids = np.array(result.boxes.cls.cpu(), dtype=\"int\")\n",
    "        # scores = np.array(result.boxes.conf.cpu(), dtype=\"float\").round(2)\n",
    "\n",
    "        return bboxes, class_ids, segmentation_contours_idx, scores\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-22T10:39:12.928818Z",
     "end_time": "2023-08-22T10:39:13.036425Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def get_max_area(indexed_box1, indexed_box2):\n",
    "    \"\"\"\n",
    "    Comparing function that will be passed to 'reduce'.\n",
    "    We calculate area of two boxes and return biggest\n",
    "    :param indexed_box1:\n",
    "    :param indexed_box2:\n",
    "    :return: one of boxes - tuple like `index, bbox`\n",
    "    \"\"\"\n",
    "    b1_i, (b1_x, b1_y, b1_x2, b1_y2) = indexed_box1\n",
    "    b2_i, (b2_x, b2_y, b2_x2, b2_y2) = indexed_box2\n",
    "    b1_area = (b1_x2 - b1_x) * (b1_y2 - b1_y)\n",
    "    b2_area = (b2_x2 - b2_x) * (b2_y2 - b2_y)\n",
    "\n",
    "    # print(f\"B1 AREA: {b1_area}\")\n",
    "    result = indexed_box1 if b1_area > b2_area else indexed_box2\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-22T10:39:12.943121Z",
     "end_time": "2023-08-22T10:39:13.052047Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def check_object(bboxes, classes):\n",
    "    proper_boxes = []\n",
    "    target_box = None, None\n",
    "    for object_class, bbox in zip(classes, bboxes):\n",
    "        # Calculate number of objects that may be cars\n",
    "        if object_class in CLASSES_IDS:\n",
    "            target_box = list(classes).index(object_class), bbox\n",
    "            proper_boxes.append(bbox)\n",
    "\n",
    "    # Now, if we have for than one car, we will continue checking\n",
    "    if len(proper_boxes) > 1:\n",
    "        # We will pick up the object with the biggest area occupied\n",
    "        target_box = reduce(get_max_area, enumerate(bboxes))\n",
    "\n",
    "    return target_box\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-22T10:39:12.958110Z",
     "end_time": "2023-08-22T10:39:13.052047Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def extract_contours(folder: str, output_dir: str, Segmentator: YOLOSegmentation, transform_to_drawings=False, skip_exist=False) -> None:\n",
    "    logs = []\n",
    "    for image_name in os.listdir(folder):\n",
    "        if not (image_name.endswith(\".jpg\") or image_name.endswith(\".jpeg\")):\n",
    "            continue\n",
    "        start_path = folder + '/' + image_name\n",
    "        end_path = f\"{output_dir}/\"\n",
    "        os.makedirs(end_path, exist_ok=True)\n",
    "        if skip_exist:\n",
    "            if image_name in os.listdir(end_path):\n",
    "                continue\n",
    "        img = cv2.imread(start_path)\n",
    "        if img is None:\n",
    "            message = f\"Can't read an image: {image_name}\"\n",
    "            print(Exception(message))\n",
    "            logs.append(message)\n",
    "            continue\n",
    "        img = cv2.resize(img, None, fx=0.5, fy=0.5)\n",
    "\n",
    "        bboxes, classes, segmentations, scores = Segmentator.detect(img)\n",
    "\n",
    "        b_id, bbox = check_object(bboxes=bboxes, classes=classes)\n",
    "        if bbox is None:\n",
    "            message = f\"Can't extract car at image: {image_name}\"\n",
    "            print(Exception(message))\n",
    "            logs.append(message)\n",
    "            continue\n",
    "        points = np.array(segmentations[b_id])\n",
    "\n",
    "        mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "\n",
    "        cv2.drawContours(mask, [points], -1, (255, 255, 255), -1, cv2.LINE_AA)\n",
    "\n",
    "        (x, y, x2, y2) = bbox\n",
    "\n",
    "\n",
    "        img = cv2.bitwise_and(img, img, mask=mask)\n",
    "\n",
    "        if transform_to_drawings:\n",
    "\n",
    "            blurred_image = cv2.GaussianBlur(img.copy(),(5,5),0)\n",
    "\n",
    "            img = cv2.Canny(blurred_image, 100, 160)\n",
    "\n",
    "\n",
    "        img = img[y:y2, x:x2]\n",
    "        cv2.imwrite(end_path + \"/\" + image_name, img)\n",
    "        time.sleep(0.01)\n",
    "    return logs\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-22T10:39:12.986182Z",
     "end_time": "2023-08-22T10:39:13.052047Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "WHOLE_DATA_PATH = \"../data/raw_in_one_folder\"\n",
    "RESULT_PATH = \"../data/processed_dataset\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-22T10:39:12.996358Z",
     "end_time": "2023-08-22T10:39:13.052047Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# for dataset in ['train', 'test']:\n",
    "#     folder = WHOLE_DATA_PATH + \"/\" + dataset\n",
    "#     result_folder = RESULT_PATH + \"/\" + dataset\n",
    "#\n",
    "#     result = extract_contours(folder=folder,\n",
    "#                      output_dir=result_folder,\n",
    "#                      Segmentator=YOLOSegmentation(model_path, device),\n",
    "#                      transform_to_drawings=False,\n",
    "#                      skip_exist=True)\n",
    "#     print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-22T10:39:12.996358Z",
     "end_time": "2023-08-22T10:39:13.052047Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 3 cars, 1502.7ms\n",
      "Speed: 31.2ms preprocess, 1502.7ms inference, 15.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 2 persons, 5 cars, 1 traffic light, 1373.9ms\n",
      "Speed: 15.7ms preprocess, 1373.9ms inference, 15.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 car, 1453.2ms\n",
      "Speed: 0.0ms preprocess, 1453.2ms inference, 15.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 car, 1 truck, 1145.4ms\n",
      "Speed: 0.0ms preprocess, 1145.4ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 1150.9ms\n",
      "Speed: 8.0ms preprocess, 1150.9ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 1179.4ms\n",
      "Speed: 1.5ms preprocess, 1179.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 car, 1436.5ms\n",
      "Speed: 15.6ms preprocess, 1436.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 cars, 1406.5ms\n",
      "Speed: 15.6ms preprocess, 1406.5ms inference, 15.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 1 car, 1 truck, 1336.3ms\n",
      "Speed: 0.0ms preprocess, 1336.3ms inference, 8.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 512x640 2 cars, 1 truck, 1554.2ms\n",
      "Speed: 0.0ms preprocess, 1554.2ms inference, 15.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 480x640 1 car, 1 train, 1415.0ms\n",
      "Speed: 0.0ms preprocess, 1415.0ms inference, 15.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 9 cars, 1 truck, 1475.0ms\n",
      "Speed: 0.0ms preprocess, 1475.0ms inference, 31.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 512x640 3 cars, 1 bus, 1520.1ms\n",
      "Speed: 0.0ms preprocess, 1520.1ms inference, 15.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 3 cars, 2 trucks, 1436.5ms\n",
      "Speed: 0.0ms preprocess, 1436.5ms inference, 15.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 1 person, 1 car, 1450.8ms\n",
      "Speed: 0.0ms preprocess, 1450.8ms inference, 15.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 480x640 11 cars, 3 trucks, 1415.5ms\n",
      "Speed: 0.0ms preprocess, 1415.5ms inference, 31.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 car, 1 bench, 1533.2ms\n",
      "Speed: 0.0ms preprocess, 1533.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 bus, 1256.7ms\n",
      "Speed: 0.0ms preprocess, 1256.7ms inference, 15.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 512x640 7 cars, 1487.6ms\n",
      "Speed: 0.0ms preprocess, 1487.6ms inference, 15.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 480x640 1 car, 1502.0ms\n",
      "Speed: 0.0ms preprocess, 1502.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 512x640 1 car, 1562.9ms\n",
      "Speed: 0.0ms preprocess, 1562.9ms inference, 15.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 448x640 4 cars, 1343.5ms\n",
      "Speed: 15.7ms preprocess, 1343.5ms inference, 15.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 car, 1421.3ms\n",
      "Speed: 15.6ms preprocess, 1421.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 car, 1315.5ms\n",
      "Speed: 15.6ms preprocess, 1315.5ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 truck, 1286.3ms\n",
      "Speed: 0.0ms preprocess, 1286.3ms inference, 0.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 1 car, 1412.5ms\n",
      "Speed: 0.0ms preprocess, 1412.5ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 car, 1380.3ms\n",
      "Speed: 0.0ms preprocess, 1380.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder = \"../data/val_dataset\"\n",
    "result_folder = \"../data/val_dataset_segmented\"\n",
    "\n",
    "extract_contours(folder=folder,\n",
    "                 output_dir=result_folder,\n",
    "                 Segmentator=YOLOSegmentation(model_path, device),\n",
    "                 transform_to_drawings=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-22T10:39:13.005140Z",
     "end_time": "2023-08-22T10:39:56.091804Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-22T10:39:56.099790Z",
     "end_time": "2023-08-22T10:39:56.131044Z"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "yolo_torch",
   "language": "python",
   "display_name": "yolo_torch"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
